{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109A Introduction to Data Science: \n",
    "\n",
    "## Homework 8  AC 209 : Trees and ensemble methods\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2018**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader\n",
    "\n",
    "<hr style=\"height:2pt\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN THIS CELL FOR FORMAT\n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b>  Question 7 </b> </div>\n",
    "\n",
    "**7.1** Describe the main differences between bagging and adaptative boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.2** Why do we use the word \"gradient\" in gradient boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.3** Describe three improvements of XGBoost over the conventional implementation of Boosted Trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b>  Question 8 </b> </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will compare some of the top ensemble methods for classification. We will look at AdaBoost, XGBoost, LGBM and CatBoost. \n",
    "\n",
    "- To install XGBoost, run `pip3 install xgboost`.\n",
    "- To install LGBM, run `pip install lightgbm`\n",
    "- To install CatBoost, run `conda -c conda-forge install catboost` if using conda, or `pip install catboost` if not. \n",
    "\n",
    "We will be using a different dataset than what we're used to, so as to test the capabilities of these advanced classifiers. We will be playing with the Forest Cover Type dataset, a classification dataset where observations from 30mx30m patches of forest are associated with the type of tree that grows there. We will be trying to predict the primary species of those patches based on 54 predictors, e.g. elevation, slope, distance to water, etc.\n",
    "\n",
    "Here are the main predictors of the dataset:\n",
    "- Elevation\n",
    "- Aspect\n",
    "- Slope\n",
    "- Horizontal_Distance_To_Hydrology \n",
    "- Vertical_Distance_To_Hydrology \n",
    "- Hillshade_9am\n",
    "- Hillshade_Noon\n",
    "- Hillshade_3pm\n",
    "- Horizontal_Distance_To_Fire_Points\n",
    "- Wilderness_Area (one-hot encoded, 4 binary columns)\n",
    "- Soil_Type (one-hot encoded, 40 binary columns)\n",
    "\n",
    "Response:\n",
    "Cover_Type (7 types), integer, 1 to 7\n",
    "\n",
    "For more details on the dataset, visit http://archive.ics.uci.edu/ml/datasets/Covertype "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.1** Import the coverage type dataset from sklearn.datasets with `datasets.fetch_covtype`. Use return_X_y=True and split the data into train and test sets (30% test). You can downsample the data to 10% of the full dataset if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.2** Train a DecisionTreeClassifier, RandomForestClassifier, AdaboostClassifier, LGBMClassifier, XGBoostClassifier, and CatBoost on the data.\n",
    "\n",
    "On a first pass, use the classifiers out of the box, with no parameter modification. As a second pass, use crossvalidation on the following parameters:\n",
    "\n",
    "- n_estimators\n",
    "- min_samples_leaf\n",
    "- max_depth\n",
    "- max_leaf_nodes\n",
    "- max_features\n",
    "\n",
    "Make sure that you use the sklearn-like interfaces:\n",
    "\n",
    "- DecisionTreeClassifier, RandomForestClassifier, AdaboostClassifier given by sklearn\n",
    "- XGBClassifier can be accessed with `from xgboost import XGBClassifier`\n",
    "- LGBMClassifier can be accessed with `from lightgbm.sklearn import LGBMClassifier`\n",
    "- CatBoostClassifier can be accessed with `from catboost import CatBoostClassifier`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.3** Time both training (.fit method) and inference (.predict method), and show classification accuracy for all classifiers. For this dataset, substract 1 to your array of labels so that the label format plays nicely with CatBoost. Comment on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.4** Let's now play with a high-dimensional dataset. Load the Faces in The Wild dataset with `datasets.fetch_lfw_people(return_X_y=True, min_faces_per_person=20)`. Split the data into train and test sets (30% test)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.5** Again, train all classifiers enumerated above and report training and inference times. Comment on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.6** How did the high dimensionality affect each classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b>  Question 7 </b> </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**7.1** Describe the main differences between bagging and adaptative boosting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Criteria|Bagging| Boosting|\n",
    "|:---:|:--------:|:---------:|\n",
    "|What data is each individual model trained on? | The training data with weightings given by a multinomial distribution with n trials and n classes | The training data with samples classified incorrectly by the previous classifier weighted higher |\n",
    "|How is the ensemble created?| Averaging or majority vote| A linear combination with better performing models weighted higher|\n",
    "|Can it be fit in parallel?| Yes| No |\n",
    "|Bias/Variance trade-off?|Increases bias|Increases variance|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**7.2** Why do we use the word \"gradient\" in gradient boosting?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boosting can be thought of as gradient descent through the prediction space or function space. For example, if our loss function is the mean squared error $L(y,\\bar{y}) = 1/n\\sum_i(y_i-\\bar{y_i}))^2$, then $\\partial L/\\partial \\bar{y_i} \\propto \\bar{y_i}-y_i$. When we fit our weak classifier to the residuals and add it to the previous model, we are essentially moving our prediction in the direction of the gradient of the loss function with respect to our previous predictions.\n",
    "\n",
    "This post is very clear on this (could be a good resource for next year):\n",
    "https://explained.ai/gradient-boosting/descent.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**7.3** Describe three improvements of XGBoost over the conventional implementation of Boosted Trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. It can be run in parallel\n",
    "2. Can handle arbitrary differentiable loss functions (makes regularization easier)\n",
    "3. Incorporates a sparsity-aware split finding algorithm to handle different types of sparsity patterns in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b>  Question 8 </b> </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will compare some of the top ensemble methods for classification. We will look at AdaBoost, XGBoost, LGBM and CatBoost. \n",
    "\n",
    "- To install XGBoost, run `pip3 install xgboost`.\n",
    "- To install LGBM, run `pip install lightgbm`\n",
    "- To install CatBoost, run `conda -c conda-forge install catboost` if using conda, or `pip install catboost` if not. \n",
    "\n",
    "We will be using a different dataset than what we're used to, so as to test the capabilities of these advanced classifiers. We will be playing with the Forest Cover Type dataset, a classification dataset where observations from 30mx30m patches of forest are associated with the type of tree that grows there. We will be trying to predict the primary species of those patches based on 54 predictors, e.g. elevation, slope, distance to water, etc.\n",
    "\n",
    "Here are the main predictors of the dataset:\n",
    "- Elevation\n",
    "- Aspect\n",
    "- Slope\n",
    "- Horizontal_Distance_To_Hydrology \n",
    "- Vertical_Distance_To_Hydrology \n",
    "- Hillshade_9am\n",
    "- Hillshade_Noon\n",
    "- Hillshade_3pm\n",
    "- Horizontal_Distance_To_Fire_Points\n",
    "- Wilderness_Area (one-hot encoded, 4 binary columns)\n",
    "- Soil_Type (one-hot encoded, 40 binary columns)\n",
    "\n",
    "Response:\n",
    "Cover_Type (7 types), integer, 1 to 7\n",
    "\n",
    "For more details on the dataset, visit http://archive.ics.uci.edu/ml/datasets/Covertype "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.1** Import the coverage type dataset from sklearn.datasets with `datasets.fetch_covtype`. Use return_X_y=True and split the data into train and test sets (30% test). You can downsample the data to 10% of the full dataset if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./covtype.csv')\n",
    "data = data.sample(frac = 0.1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('Cover_Type', axis = 1), data['Cover_Type'], test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.2** Train a DecisionTreeClassifier, RandomForestClassifier, AdaboostClassifier, LGBMClassifier, XGBoostClassifier, and CatBoost on the data.\n",
    "\n",
    "On a first pass, use the classifiers out of the box, with no parameter modification. As a second pass, use crossvalidation on the following parameters:\n",
    "\n",
    "- n_estimators\n",
    "- min_samples_leaf\n",
    "- max_depth\n",
    "- max_leaf_nodes\n",
    "- max_features\n",
    "\n",
    "Make sure that you use the sklearn-like interfaces:\n",
    "\n",
    "- DecisionTreeClassifier, RandomForestClassifier, AdaboostClassifier given by sklearn\n",
    "- XGBClassifier can be accessed with `from xgboost import XGBClassifier`\n",
    "- LGBMClassifier can be accessed with `from lightgbm.sklearn import LGBMClassifier`\n",
    "- CatBoostClassifier can be accessed with `from catboost import CatBoostClassifier`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joshfeldman/anaconda3/envs/py36/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.1) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# I'm sorry, I couldn't get catboost to install on my machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.tree.tree.DecisionTreeClassifier'>\n",
      "<class 'sklearn.ensemble.forest.RandomForestClassifier'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joshfeldman/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'>\n",
      "<class 'xgboost.sklearn.XGBClassifier'>\n",
      "<class 'lightgbm.sklearn.LGBMClassifier'>\n"
     ]
    }
   ],
   "source": [
    "models = [DecisionTreeClassifier, \n",
    "          RandomForestClassifier, \n",
    "          AdaBoostClassifier,\n",
    "          XGBClassifier,\n",
    "          LGBMClassifier,\n",
    "          ]\n",
    "trained_vanilla_models = []\n",
    "for m in models:\n",
    "    print(m)\n",
    "    trained_vanilla_models.append(m().fit(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "             splitter='best'),\n",
       " RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "             oob_score=False, random_state=None, verbose=0,\n",
       "             warm_start=False),\n",
       " AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "           learning_rate=1.0, n_estimators=50, random_state=None),\n",
       " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "        colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "        max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "        n_jobs=1, nthread=None, objective='multi:softprob', random_state=0,\n",
       "        reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "        silent=True, subsample=1),\n",
       " LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "         importance_type='split', learning_rate=0.1, max_depth=-1,\n",
       "         min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "         n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
       "         random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "         subsample=1.0, subsample_for_bin=200000, subsample_freq=0)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_vanilla_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [200, 300], \n",
    "    'min_samples_leaf': [1,5],\n",
    "    'max_depth': [1,5],\n",
    "    'max_leaf_nodes': [8,16],\n",
    "    'max_features': [0.5,1]\n",
    "}\n",
    "\n",
    "models = [DecisionTreeClassifier, \n",
    "          RandomForestClassifier, \n",
    "          AdaBoostClassifier,\n",
    "          XGBClassifier,\n",
    "          LGBMClassifier,\n",
    "         ]\n",
    "\n",
    "trained_cv_models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.tree.tree.DecisionTreeClassifier'>\n",
      "<class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "running multiple jobs\n",
      "<class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'>\n",
      "<class 'xgboost.sklearn.XGBClassifier'>\n",
      "running multiple jobs\n",
      "<class 'lightgbm.sklearn.LGBMClassifier'>\n",
      "running multiple jobs\n"
     ]
    }
   ],
   "source": [
    "for m in models:\n",
    "    print(m)\n",
    "    try:\n",
    "        model = m(n_jobs = 8)\n",
    "        print('running multiple jobs')\n",
    "    except TypeError:\n",
    "        model = m()\n",
    "    params = {k : param_grid[k] for k in param_grid if k in model.get_params()}\n",
    "    m_cv = GridSearchCV(model, params, cv = 3)\n",
    "    trained_cv_models.append(m_cv.fit(X_train, y_train))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "        estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "             splitter='best'),\n",
       "        fit_params=None, iid='warn', n_jobs=None,\n",
       "        param_grid={'min_samples_leaf': [1, 5], 'max_depth': [1, 5], 'max_leaf_nodes': [8, 16], 'max_features': [0.5, 1]},\n",
       "        pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "        scoring=None, verbose=0),\n",
       " GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "        estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "             splitter='best'),\n",
       "        fit_params=None, iid='warn', n_jobs=None,\n",
       "        param_grid={'min_samples_leaf': [1, 5], 'max_depth': [1, 5], 'max_leaf_nodes': [8, 16], 'max_features': [0.5, 1]},\n",
       "        pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "        scoring=None, verbose=0),\n",
       " GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "        estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "             splitter='best'),\n",
       "        fit_params=None, iid='warn', n_jobs=None,\n",
       "        param_grid={'min_samples_leaf': [1, 5], 'max_depth': [1, 5], 'max_leaf_nodes': [8, 16], 'max_features': [0.5, 1]},\n",
       "        pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "        scoring=None, verbose=0),\n",
       " GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "        estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=8,\n",
       "             oob_score=False, random_state=None, verbose=0,\n",
       "             warm_start=False),\n",
       "        fit_params=None, iid='warn', n_jobs=None,\n",
       "        param_grid={'n_estimators': [200, 300], 'min_samples_leaf': [1, 5], 'max_depth': [1, 5], 'max_leaf_nodes': [8, 16], 'max_features': [0.5, 1]},\n",
       "        pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "        scoring=None, verbose=0),\n",
       " GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "        estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "           learning_rate=1.0, n_estimators=50, random_state=None),\n",
       "        fit_params=None, iid='warn', n_jobs=None,\n",
       "        param_grid={'n_estimators': [200, 300]}, pre_dispatch='2*n_jobs',\n",
       "        refit=True, return_train_score='warn', scoring=None, verbose=0),\n",
       " GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "        estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "        colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "        max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "        n_jobs=8, nthread=None, objective='binary:logistic', random_state=0,\n",
       "        reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "        silent=True, subsample=1),\n",
       "        fit_params=None, iid='warn', n_jobs=None,\n",
       "        param_grid={'n_estimators': [200, 300], 'max_depth': [1, 5]},\n",
       "        pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "        scoring=None, verbose=0),\n",
       " GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "        estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "         importance_type='split', learning_rate=0.1, max_depth=-1,\n",
       "         min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "         n_estimators=100, n_jobs=8, num_leaves=31, objective=None,\n",
       "         random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "         subsample=1.0, subsample_for_bin=200000, subsample_freq=0),\n",
       "        fit_params=None, iid='warn', n_jobs=None,\n",
       "        param_grid={'n_estimators': [200, 300], 'max_depth': [1, 5]},\n",
       "        pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "        scoring=None, verbose=0)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_cv_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.3** Time both training (.fit method) and inference (.predict method), and show classification accuracy for all classifiers. For this dataset, substract 1 to your array of labels so that the label format plays nicely with CatBoost. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.tree.tree.DecisionTreeClassifier'>\n",
      "<class 'sklearn.ensemble.forest.RandomForestClassifier'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joshfeldman/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'>\n",
      "<class 'xgboost.sklearn.XGBClassifier'>\n",
      "<class 'lightgbm.sklearn.LGBMClassifier'>\n"
     ]
    }
   ],
   "source": [
    "models = [DecisionTreeClassifier, \n",
    "          RandomForestClassifier, \n",
    "          AdaBoostClassifier,\n",
    "          XGBClassifier,\n",
    "          LGBMClassifier,\n",
    "          ]\n",
    "\n",
    "trained_vanilla_models = []\n",
    "time = []\n",
    "for m in models:\n",
    "    print(m)\n",
    "    start_time = timeit.default_timer()\n",
    "    trained_vanilla_models.append(m().fit(X_train, y_train))\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    time.append(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{sklearn.tree.tree.DecisionTreeClassifier: 0.5296440760139376,\n",
       " sklearn.ensemble.forest.RandomForestClassifier: 0.622468160931021,\n",
       " sklearn.ensemble.weight_boosting.AdaBoostClassifier: 1.611194389872253,\n",
       " xgboost.sklearn.XGBClassifier: 68.43641737685539,\n",
       " lightgbm.sklearn.LGBMClassifier: 3.465232075890526}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"training time\")\n",
    "print(dict(zip(models,time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=1, nthread=None, objective='multi:softprob', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "        importance_type='split', learning_rate=0.1, max_depth=-1,\n",
      "        min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "        n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
      "        random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
      "        subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n"
     ]
    }
   ],
   "source": [
    "train_accs = []\n",
    "test_accs = []\n",
    "time_predict_train = []\n",
    "time_predict_test = []\n",
    "\n",
    "for m in trained_vanilla_models:\n",
    "    print(m)\n",
    "    \n",
    "    # predict train\n",
    "    start_time = timeit.default_timer()\n",
    "    y_train_pred = m.predict(X_train)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    time_predict_train.append(elapsed)\n",
    "    train_accs.append(accuracy_score(y_train, y_train_pred))\n",
    "\n",
    "    # predict test\n",
    "    start_time = timeit.default_timer()\n",
    "    y_test_pred = m.predict(X_test)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    time_predict_test.append(elapsed)\n",
    "    test_accs.append(accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred time training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{sklearn.tree.tree.DecisionTreeClassifier: 0.0161871500313282,\n",
       " sklearn.ensemble.forest.RandomForestClassifier: 0.11870791390538216,\n",
       " sklearn.ensemble.weight_boosting.AdaBoostClassifier: 0.24576081591658294,\n",
       " xgboost.sklearn.XGBClassifier: 0.9766346090473235,\n",
       " lightgbm.sklearn.LGBMClassifier: 0.5447103709448129}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"pred time training\")\n",
    "dict(zip(models,time_predict_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{sklearn.tree.tree.DecisionTreeClassifier: 1.0,\n",
       " sklearn.ensemble.forest.RandomForestClassifier: 0.9937791984263585,\n",
       " sklearn.ensemble.weight_boosting.AdaBoostClassifier: 0.6329727071551512,\n",
       " xgboost.sklearn.XGBClassifier: 0.7517580526186378,\n",
       " lightgbm.sklearn.LGBMClassifier: 0.8636587164986477}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"training accuracy\")\n",
    "dict(zip(models, train_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred time testing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{sklearn.tree.tree.DecisionTreeClassifier: 0.011219922918826342,\n",
       " sklearn.ensemble.forest.RandomForestClassifier: 0.03834013803862035,\n",
       " sklearn.ensemble.weight_boosting.AdaBoostClassifier: 0.10123721696436405,\n",
       " xgboost.sklearn.XGBClassifier: 0.4385811679530889,\n",
       " lightgbm.sklearn.LGBMClassifier: 0.20654238597489893}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"pred time testing\")\n",
    "dict(zip(models,time_predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{sklearn.tree.tree.DecisionTreeClassifier: 0.820434857437898,\n",
       " sklearn.ensemble.forest.RandomForestClassifier: 0.8527910045321554,\n",
       " sklearn.ensemble.weight_boosting.AdaBoostClassifier: 0.6299695944007803,\n",
       " xgboost.sklearn.XGBClassifier: 0.7465435144283173,\n",
       " lightgbm.sklearn.LGBMClassifier: 0.8239343698009294}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"test accuracy\")\n",
    "dict(zip(models, test_accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n",
    "\n",
    "The boosting methods are slower to train, particularly xgboost. Boosting methods are also slower in predictions. Out-of-the-box, Random forest had the best test set accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.4** Let's now play with a high-dimensional dataset. Load the Faces in The Wild dataset with `datasets.fetch_lfw_people(return_X_y=True, min_faces_per_person=20)`. Split the data into train and test sets (30% test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading LFW metadata: https://ndownloader.figshare.com/files/5976012\n",
      "Downloading LFW metadata: https://ndownloader.figshare.com/files/5976009\n",
      "Downloading LFW metadata: https://ndownloader.figshare.com/files/5976006\n",
      "Downloading LFW data (~200MB): https://ndownloader.figshare.com/files/5976015\n"
     ]
    }
   ],
   "source": [
    "data = datasets.fetch_lfw_people(return_X_y=True, min_faces_per_person=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[0], data[1], test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.5** Again, train all classifiers enumerated above and report training and inference times. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.tree.tree.DecisionTreeClassifier'>\n",
      "<class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "<class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'>\n",
      "<class 'xgboost.sklearn.XGBClassifier'>\n",
      "<class 'lightgbm.sklearn.LGBMClassifier'>\n"
     ]
    }
   ],
   "source": [
    "models = [DecisionTreeClassifier, \n",
    "          RandomForestClassifier, \n",
    "          AdaBoostClassifier,\n",
    "          XGBClassifier,\n",
    "          LGBMClassifier,\n",
    "          ]\n",
    "\n",
    "trained_vanilla_models = []\n",
    "time = []\n",
    "for m in models:\n",
    "    print(m)\n",
    "    try:\n",
    "        start_time = timeit.default_timer()\n",
    "        trained_vanilla_models.append(m(n_estimators = 50).fit(X_train, y_train))\n",
    "        elapsed = timeit.default_timer() - start_time\n",
    "    except TypeError:\n",
    "        start_time = timeit.default_timer()\n",
    "        trained_vanilla_models.append(m().fit(X_train, y_train))\n",
    "        elapsed = timeit.default_timer() - start_time\n",
    "    time.append(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{sklearn.tree.tree.DecisionTreeClassifier: 9.668490819865838,\n",
       " sklearn.ensemble.forest.RandomForestClassifier: 5.625482650008053,\n",
       " sklearn.ensemble.weight_boosting.AdaBoostClassifier: 24.437815509038046,\n",
       " xgboost.sklearn.XGBClassifier: 957.7722799461335,\n",
       " lightgbm.sklearn.LGBMClassifier: 594.304289652966}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"training time\")\n",
    "dict(zip(models,time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=50,\n",
      "       n_jobs=1, nthread=None, objective='multi:softprob', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "        importance_type='split', learning_rate=0.1, max_depth=-1,\n",
      "        min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "        n_estimators=50, n_jobs=-1, num_leaves=31, objective=None,\n",
      "        random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
      "        subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n"
     ]
    }
   ],
   "source": [
    "train_accs = []\n",
    "test_accs = []\n",
    "time_predict_train = []\n",
    "time_predict_test = []\n",
    "\n",
    "for m in trained_vanilla_models:\n",
    "    print(m)\n",
    "    \n",
    "    # predict train\n",
    "    start_time = timeit.default_timer()\n",
    "    y_train_pred = m.predict(X_train)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    time_predict_train.append(elapsed)\n",
    "    train_accs.append(accuracy_score(y_train, y_train_pred))\n",
    "\n",
    "    # predict test\n",
    "    start_time = timeit.default_timer()\n",
    "    y_test_pred = m.predict(X_test)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    time_predict_test.append(elapsed)\n",
    "    test_accs.append(accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred time training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{sklearn.tree.tree.DecisionTreeClassifier: 0.0058713448233902454,\n",
       " sklearn.ensemble.forest.RandomForestClassifier: 0.11207350599579513,\n",
       " sklearn.ensemble.weight_boosting.AdaBoostClassifier: 0.17464763019233942,\n",
       " xgboost.sklearn.XGBClassifier: 5.322163850069046,\n",
       " lightgbm.sklearn.LGBMClassifier: 0.4039787990041077}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"pred time training\")\n",
    "dict(zip(models,time_predict_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{sklearn.tree.tree.DecisionTreeClassifier: 1.0,\n",
       " sklearn.ensemble.forest.RandomForestClassifier: 1.0,\n",
       " sklearn.ensemble.weight_boosting.AdaBoostClassifier: 0.19801512287334594,\n",
       " xgboost.sklearn.XGBClassifier: 0.999054820415879,\n",
       " lightgbm.sklearn.LGBMClassifier: 1.0}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"training accuracy\")\n",
    "dict(zip(models, train_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred time testing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{sklearn.tree.tree.DecisionTreeClassifier: 0.0022696589585393667,\n",
       " sklearn.ensemble.forest.RandomForestClassifier: 0.02217451692558825,\n",
       " sklearn.ensemble.weight_boosting.AdaBoostClassifier: 0.06137705501168966,\n",
       " xgboost.sklearn.XGBClassifier: 2.262235410977155,\n",
       " lightgbm.sklearn.LGBMClassifier: 0.18916428904049098}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"pred time testing\")\n",
    "dict(zip(models,time_predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{sklearn.tree.tree.DecisionTreeClassifier: 0.19625137816979052,\n",
       " sklearn.ensemble.forest.RandomForestClassifier: 0.3539140022050717,\n",
       " sklearn.ensemble.weight_boosting.AdaBoostClassifier: 0.17861080485115766,\n",
       " xgboost.sklearn.XGBClassifier: 0.40352811466372657,\n",
       " lightgbm.sklearn.LGBMClassifier: 0.38257993384785005}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"test accuracy\")\n",
    "dict(zip(models, test_accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n",
    "**Comments:**\n",
    "* The boosting methods are slower to train than the others\n",
    "* XGBoost has the slowest prediction time (this is strange because one of the advantages of xgboost is that it's fast...I think I'm not using it correctly) I've read that XGBoost isn't great with multiclass classification, so maybe that's what's going on\n",
    "* XGBoost performs best out of all the classifiers, withg lightGBM and random forest being 2nd and 3rd best, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.6** How did the high dimensionality affect each classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the classifiers got worse, with xgboost and lightgbm being the most resilient."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
