{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109A Introduction to Data Science: \n",
    "\n",
    "## Homework 8  AC 209 : Trees and ensemble methods\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2018**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader\n",
    "\n",
    "<hr style=\"height:2pt\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN THIS CELL FOR FORMAT\n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b>  Question 7 </b> </div>\n",
    "\n",
    "**7.1** Describe the main differences between bagging and adaptative boosting.\n",
    "\n",
    "**7.2** Why do we use the word \"gradient\" in gradient boosting?\n",
    "\n",
    "**7.3** Describe three improvements of XGBoost over the conventional implementation of Boosted Trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b>  Question 8 </b> </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will compare some of the top ensemble methods for classification. We will look at AdaBoost, XGBoost, LGBM and CatBoost. \n",
    "\n",
    "- To install XGBoost, run `pip3 install xgboost`.\n",
    "- To install LGBM, run `pip install lightgbm`\n",
    "- To install CatBoost, run `conda -c conda-forge install catboost` if using conda, or `pip install catboost` if not. \n",
    "\n",
    "We will be using a different dataset than what we're used to, so as to test the capabilities of these advanced classifiers. We will be playing with the Forest Cover Type dataset, a classification dataset where observations from 30mx30m patches of forest are associated with the type of tree that grows there. We will be trying to predict the primary species of those patches based on 54 predictors, e.g. elevation, slope, distance to water, etc.\n",
    "\n",
    "Here are the main predictors of the dataset:\n",
    "- Elevation\n",
    "- Aspect\n",
    "- Slope\n",
    "- Horizontal_Distance_To_Hydrology \n",
    "- Vertical_Distance_To_Hydrology \n",
    "- Hillshade_9am\n",
    "- Hillshade_Noon\n",
    "- Hillshade_3pm\n",
    "- Horizontal_Distance_To_Fire_Points\n",
    "- Wilderness_Area (one-hot encoded, 4 binary columns)\n",
    "- Soil_Type (one-hot encoded, 40 binary columns)\n",
    "\n",
    "Response:\n",
    "Cover_Type (7 types), integer, 1 to 7\n",
    "\n",
    "For more details on the dataset, visit http://archive.ics.uci.edu/ml/datasets/Covertype "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.1** Import the coverage type dataset from sklearn.datasets with `datasets.fetch_covtype`. Use return_X_y=True and split the data into train and test sets (30% test). You can downsample the data to 10% of the full dataset if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.2** Train a DecisionTreeClassifier, RandomForestClassifier, AdaboostClassifier, LGBMClassifier, XGBoostClassifier, and CatBoost on the data.\n",
    "\n",
    "On a first pass, use the classifiers out of the box, with no parameter modification. As a second pass, use crossvalidation on the following parameters:\n",
    "\n",
    "- n_estimators\n",
    "- min_samples_leaf\n",
    "- max_depth\n",
    "- max_leaf_nodes\n",
    "- max_features\n",
    "\n",
    "Make sure that you use the sklearn-like interfaces:\n",
    "\n",
    "- DecisionTreeClassifier, RandomForestClassifier, AdaboostClassifier given by sklearn\n",
    "- XGBClassifier can be accessed with `from xgboost import XGBClassifier`\n",
    "- LGBMClassifier can be accessed with `from lightgbm.sklearn import LGBMClassifier`\n",
    "- CatBoostClassifier can be accessed with `from catboost import CatBoostClassifier`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.3** Time both training (.fit method) and inference (.predict method), and show classification accuracy for all classifiers. For this dataset, substract 1 to your array of labels so that the label format plays nicely with CatBoost. Comment on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.4** Let's now play with a high-dimensional dataset. Load the Faces in The Wild dataset with `datasets.fetch_lfw_people(return_X_y=True, min_faces_per_person=20)`. Split the data into train and test sets (30% test)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.5** Again, train all classifiers enumerated above and report training and inference times. Comment on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.6** How did the high dimensionality affect each classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b>  Question 7 </b> </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**7.1** Describe the main differences between bagging and adaptative boosting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**7.2** Why do we use the word \"gradient\" in gradient boosting?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**7.3** Describe three improvements of XGBoost over the conventional implementation of Boosted Trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b>  Question 8 </b> </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will compare some of the top ensemble methods for classification. We will look at AdaBoost, XGBoost, LGBM and CatBoost. \n",
    "\n",
    "- To install XGBoost, run `pip3 install xgboost`.\n",
    "- To install LGBM, run `pip install lightgbm`\n",
    "- To install CatBoost, run `conda -c conda-forge install catboost` if using conda, or `pip install catboost` if not. \n",
    "\n",
    "We will be using a different dataset than what we're used to, so as to test the capabilities of these advanced classifiers. We will be playing with the Forest Cover Type dataset, a classification dataset where observations from 30mx30m patches of forest are associated with the type of tree that grows there. We will be trying to predict the primary species of those patches based on 54 predictors, e.g. elevation, slope, distance to water, etc.\n",
    "\n",
    "Here are the main predictors of the dataset:\n",
    "- Elevation\n",
    "- Aspect\n",
    "- Slope\n",
    "- Horizontal_Distance_To_Hydrology \n",
    "- Vertical_Distance_To_Hydrology \n",
    "- Hillshade_9am\n",
    "- Hillshade_Noon\n",
    "- Hillshade_3pm\n",
    "- Horizontal_Distance_To_Fire_Points\n",
    "- Wilderness_Area (one-hot encoded, 4 binary columns)\n",
    "- Soil_Type (one-hot encoded, 40 binary columns)\n",
    "\n",
    "Response:\n",
    "Cover_Type (7 types), integer, 1 to 7\n",
    "\n",
    "For more details on the dataset, visit http://archive.ics.uci.edu/ml/datasets/Covertype "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.1** Import the coverage type dataset from sklearn.datasets with `datasets.fetch_covtype`. Use return_X_y=True and split the data into train and test sets (30% test). You can downsample the data to 10% of the full dataset if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.2** Train a DecisionTreeClassifier, RandomForestClassifier, AdaboostClassifier, LGBMClassifier, XGBoostClassifier, and CatBoost on the data.\n",
    "\n",
    "On a first pass, use the classifiers out of the box, with no parameter modification. As a second pass, use crossvalidation on the following parameters:\n",
    "\n",
    "- n_estimators\n",
    "- min_samples_leaf\n",
    "- max_depth\n",
    "- max_leaf_nodes\n",
    "- max_features\n",
    "\n",
    "Make sure that you use the sklearn-like interfaces:\n",
    "\n",
    "- DecisionTreeClassifier, RandomForestClassifier, AdaboostClassifier given by sklearn\n",
    "- XGBClassifier can be accessed with `from xgboost import XGBClassifier`\n",
    "- LGBMClassifier can be accessed with `from lightgbm.sklearn import LGBMClassifier`\n",
    "- CatBoostClassifier can be accessed with `from catboost import CatBoostClassifier`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.3** Time both training (.fit method) and inference (.predict method), and show classification accuracy for all classifiers. For this dataset, substract 1 to your array of labels so that the label format plays nicely with CatBoost. Comment on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.4** Let's now play with a high-dimensional dataset. Load the Faces in The Wild dataset with `datasets.fetch_lfw_people(return_X_y=True, min_faces_per_person=20)`. Split the data into train and test sets (30% test)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.5** Again, train all classifiers enumerated above and report training and inference times. Comment on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.6** How did the high dimensionality affect each classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
