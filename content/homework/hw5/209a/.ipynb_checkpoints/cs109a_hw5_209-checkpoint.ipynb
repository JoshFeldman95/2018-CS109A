{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109A Introduction to Data Science: \n",
    "\n",
    "## Homework 5  AC 209 : PCA\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2018**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader\n",
    "\n",
    "Names of people you have worked with goes here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN THIS CELL FOR FORMAT\n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b> Question 5 [25 pts] </b> </div>\n",
    "\n",
    "Suppose we want to conduct PCA on the model matrix $X \\in \\Re^{n√óp}$, where the columns have been suitably set to zero mean. In this question, we consider the squared reconstruction error:\n",
    "\n",
    "$$  \\parallel XQ- XQ_m \\parallel ^2 $$\n",
    "\n",
    "for a suitable set of eigenvectors forming the matrix $Q_m$, as discussed below. Suppose that we conduct eigendecomposition of $X^T X$ and obtain eigenvalues $\\lambda_1, \\ldots , \\lambda_p$ and principal components $Q$, i.e.\n",
    "\n",
    "$$ X^T X = Q \\Lambda Q ^T $$\n",
    "\n",
    "**5.1** Suppose that the matrix norm is simply the squared dot product, namely\n",
    "\n",
    "$$ \\parallel A \\parallel ^2 = A^T A $$\n",
    "\n",
    "Then, express the reconstruction error as a sum of matrix products.\n",
    "\n",
    "**5.2**  Simplify your result from 5.1 based on properties of the matrices $Q$.\n",
    "\n",
    "**5.3** Now let $Q_m$ be the matrix of the first $m < p$ eigenvectors, namely\n",
    "\n",
    "$$ Q_m = (q_1, \\ldots, q_m, 0, \\ldots, 0) \\in \\Re^{p \\times p} $$\n",
    "\n",
    "Thus, $X Q_m$ is the PCA projection of the data into the space spanned by the first $m$ principal components. Express the products $Q^T_m Q$ and $Q^T Q_m$, again using properties of the eigenbasis $q_1, \\ldots, q_p$.\n",
    "\n",
    "**5.4**  Use your results from 5.3 to finally fully simplify your expression from 5.2.\n",
    "\n",
    "**5.5** Note that the result you obtain should still be a matrix, i.e. this does not define a proper norm on the space of matrices (since the value should be a scalar). Consequently, the true matrix norm is actually the trace of the\n",
    "above result, namely\n",
    "$$ \\parallel A \\parallel ^2  = {\\rm trace} (A^T A) $$\n",
    "Use your result from 5.4 and this new definition to find a simple expression\n",
    "for the reconstruction error in terms of the eigenvalues.\n",
    "\n",
    "**5.6** Interpret your result from (5). In light of your results, does our procedure for PCA (selecting the $m$ substantially larger eigenvalues) make sense? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.1** Suppose that the matrix norm is simply the squared dot product, namely\n",
    "\n",
    "$$ \\parallel A \\parallel ^2 = A^T A $$\n",
    "\n",
    "Then, express the reconstruction error as a sum of matrix products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n",
    "$$\n",
    "\\begin{align}\n",
    "\\parallel XQ- XQ_m \\parallel ^2 &= (XQ- XQ_m)^{T}(XQ- XQ_m)\\\\\n",
    "&= (Q^TX^T- Q_m^TX^T)(XQ- XQ_m) \\\\\n",
    "&= Q^TX^TXQ- Q_m^TX^TXQ- Q^TX^TXQ_m+ Q_m^TX^TXQ_m\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.2**  Simplify your result from 5.1 based on properties of the matrices $Q$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n",
    "$$\n",
    "\\begin{align}\n",
    "Q^TX^TXQ- Q_m^TX^TXQ- Q^TX^TXQ_m+ Q_m^TX^TXQ_m &= Q^TQ \\Lambda Q ^TQ- Q_m^TQ \\Lambda Q ^TQ- Q^TQ \\Lambda Q ^TQ_m+ Q_m^TQ \\Lambda Q ^TQ_m  \\\\\n",
    "&= \\Lambda  - Q_m^TQ \\Lambda - \\Lambda Q ^TQ_m+ Q_m^TQ \\Lambda Q ^TQ_m  \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.3** Now let $Q_m$ be the matrix of the first $m < p$ eigenvectors, namely\n",
    "\n",
    "$$ Q_m = (q_1, \\ldots, q_m, 0, \\ldots, 0) \\in \\Re^{p \\times p} $$\n",
    "\n",
    "Thus, $X Q_m$ is the PCA projection of the data into the space spanned by the first $m$ principal components. Express the products $Q^T_m Q$ and $Q^T Q_m$, again using properties of the eigenbasis $q_1, \\ldots, q_p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here* \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Q^T_m Q &= (q_1, \\ldots, q_m, 0, \\ldots, 0)^T(q_1, \\ldots, q_p)\\\\\n",
    "&= (v_1, \\ldots, v_m, 0, \\ldots, 0) \\\\\n",
    "&= I_m\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $v_i$ is a vector of length $p$ with a $1$ in the $i^{th}$ component.\n",
    "$$\n",
    "\\begin{align}\n",
    "Q^T Q_m &= (Q^T_m Q)^T\\\\\n",
    "&= I_m^T \\\\\n",
    "&= I_m \\\\\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.4**  Use your results from 5.3 to finally fully simplify your expression from 5.2.\n",
    "\n",
    "*your answer here* \n",
    "$$\n",
    "\\begin{align}\n",
    "\\Lambda  - Q_m^TQ \\Lambda - \\Lambda Q ^TQ_m+ Q_m^TQ \\Lambda Q ^TQ_m &= \\Lambda  - I_m\\Lambda - \\Lambda I_m+ I_m \\Lambda I_m \\\\\n",
    "&= \\Lambda  - \\Lambda_m - \\Lambda_m+ \\Lambda_m \\\\\n",
    "&= \\Lambda  - \\Lambda_m\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\Lambda_m$ is a matrix of zeros with the first $m$ eigenvalues on the diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.5** Note that the result you obtain should still be a matrix, i.e. this does not define a proper norm on the space of matrices (since the value should be a scalar). Consequently, the true matrix norm is actually the trace of the\n",
    "above result, namely\n",
    "$$ \\parallel A \\parallel ^2  = {\\rm trace} (A^T A) $$\n",
    "Use your result from 5.4 and this new definition to find a simple expression\n",
    "for the reconstruction error in terms of the eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here* \n",
    "\n",
    "The true matrix norm is \n",
    "\n",
    "$$\n",
    "\\parallel XQ- XQ_m \\parallel ^2  = {\\rm trace} (\\Lambda  - \\Lambda_m) = \\sum_{i=m+1}^p \\lambda_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.6** Interpret your result from (5). In light of your results, does our procedure for PCA (selecting the $m$ substantially larger eigenvalues) make sense? Why or why not?\n",
    "\n",
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting the eigenvectors associated with the $m$ largest eigenvalues is the best choice because, out of any selection of $m$ eigenvectors, this minimizes the reconstruction error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
